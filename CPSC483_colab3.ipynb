{
 "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Graph-and-Geometric-Learning/CPSC483-colab/blob/main/CPSC483_colab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "id": "7dd35d72",
   "metadata": {},
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8988c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu111.html\n",
    "!pip install torch-geometric\n",
    "!pip install ogb  # for datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b3402",
   "metadata": {},
   "source": [
    "# Data piepline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db198a",
   "metadata": {},
   "source": [
    "## use cython to accelerate data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42369b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import cython\n",
    "from cython.parallel cimport prange, parallel\n",
    "cimport numpy\n",
    "import numpy\n",
    "\n",
    "def floyd_warshall(adjacency_matrix):\n",
    "\n",
    "    (nrows, ncols) = adjacency_matrix.shape\n",
    "    assert nrows == ncols\n",
    "    cdef unsigned int n = nrows\n",
    "\n",
    "    adj_mat_copy = adjacency_matrix.astype(long, order='C', casting='safe', copy=True)\n",
    "    assert adj_mat_copy.flags['C_CONTIGUOUS']\n",
    "    cdef numpy.ndarray[long, ndim=2, mode='c'] M = adj_mat_copy\n",
    "    cdef numpy.ndarray[long, ndim=2, mode='c'] path = numpy.zeros([n, n], dtype=numpy.int64)\n",
    "\n",
    "    cdef unsigned int i, j, k\n",
    "    cdef long M_ij, M_ik, cost_ikkj\n",
    "    cdef long* M_ptr = &M[0,0]\n",
    "    cdef long* M_i_ptr\n",
    "    cdef long* M_k_ptr\n",
    "\n",
    "    # set unreachable nodes distance to 510\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                M[i][j] = 0\n",
    "            elif M[i][j] == 0:\n",
    "                M[i][j] = 510\n",
    "\n",
    "    # floyed algo\n",
    "    for k in range(n):\n",
    "        M_k_ptr = M_ptr + n*k\n",
    "        for i in range(n):\n",
    "            M_i_ptr = M_ptr + n*i\n",
    "            M_ik = M_i_ptr[k]\n",
    "            for j in range(n):\n",
    "                cost_ikkj = M_ik + M_k_ptr[j]\n",
    "                M_ij = M_i_ptr[j]\n",
    "                if M_ij > cost_ikkj:\n",
    "                    M_i_ptr[j] = cost_ikkj\n",
    "                    path[i][j] = k\n",
    "\n",
    "    # set unreachable path to 510\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if M[i][j] >= 510:\n",
    "                path[i][j] = 510\n",
    "                M[i][j] = 510\n",
    "\n",
    "    return M, path\n",
    "\n",
    "\n",
    "def get_all_edges(path, i, j):\n",
    "    cdef unsigned int k = path[i][j]\n",
    "    if k == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return get_all_edges(path, i, k) + [k] + get_all_edges(path, k, j)\n",
    "\n",
    "\n",
    "def gen_edge_input(max_dist, path, edge_feat):\n",
    "\n",
    "    (nrows, ncols) = path.shape\n",
    "    assert nrows == ncols\n",
    "    cdef unsigned int n = nrows\n",
    "    cdef unsigned int max_dist_copy = max_dist\n",
    "\n",
    "    path_copy = path.astype(long, order='C', casting='safe', copy=True)\n",
    "    edge_feat_copy = edge_feat.astype(long, order='C', casting='safe', copy=True)\n",
    "    assert path_copy.flags['C_CONTIGUOUS']\n",
    "    assert edge_feat_copy.flags['C_CONTIGUOUS']\n",
    "\n",
    "    cdef numpy.ndarray[long, ndim=4, mode='c'] edge_fea_all = -1 * numpy.ones([n, n, max_dist_copy, edge_feat.shape[-1]], dtype=numpy.int64)\n",
    "    cdef unsigned int i, j, k, num_path, cur\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if path_copy[i][j] == 510:\n",
    "                continue\n",
    "            # if path_copy[i][j] == 0:\n",
    "            #     continue\n",
    "            path = [i] + get_all_edges(path_copy, i, j) + [j]\n",
    "            # path = [i] + [j]\n",
    "            num_path = len(path) - 1\n",
    "            for k in range(num_path):\n",
    "                edge_fea_all[i, j, k, :] = edge_feat_copy[path[k], path[k+1], :]\n",
    "\n",
    "    return edge_fea_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cae808",
   "metadata": {},
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# import pyximport\n",
    "# pyximport.install(setup_args={'include_dirs': np.get_include()})\n",
    "# import algos\n",
    "\n",
    "\n",
    "def convert_to_single_emb(x, offset=512):\n",
    "    # for Graphormer\n",
    "    feature_num = x.size(1) if len(x.size()) > 1 else 1\n",
    "    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n",
    "    x = x + feature_offset\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_item(item):\n",
    "    # for Graphormer\n",
    "    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n",
    "\n",
    "    N = x.size(0)\n",
    "    x = convert_to_single_emb(x)\n",
    "\n",
    "    # node adj matrix [N, N] bool\n",
    "    adj = torch.zeros([N, N], dtype=torch.bool)\n",
    "    adj[edge_index[0, :], edge_index[1, :]] = True\n",
    "\n",
    "    # edge feature here\n",
    "    if len(edge_attr.size()) == 1:\n",
    "        edge_attr = edge_attr[:, None]\n",
    "    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n",
    "    attn_edge_type[edge_index[0, :], edge_index[1, :]] = convert_to_single_emb(edge_attr) + 1\n",
    "\n",
    "    shortest_path_result, path = floyd_warshall(adj.numpy())\n",
    "    max_dist = np.amax(shortest_path_result)\n",
    "    # max_dist = 1\n",
    "    edge_input = gen_edge_input(max_dist, path, attn_edge_type.numpy())\n",
    "    # edge_input = algos.gen_edge_input(max_dist, adj.numpy(), attn_edge_type.numpy())\n",
    "    rel_pos = torch.from_numpy((shortest_path_result)).long()\n",
    "    # rel_pos = torch.from_numpy((adj.numpy())).long()\n",
    "    attn_bias = torch.zeros(\n",
    "        [N + 1, N + 1], dtype=torch.float)  # with graph token\n",
    "\n",
    "    # combine\n",
    "    item.x = x\n",
    "    item.adj = adj\n",
    "    item.attn_bias = attn_bias\n",
    "    item.attn_edge_type = attn_edge_type\n",
    "    item.rel_pos = rel_pos\n",
    "    item.in_degree = adj.long().sum(dim=1).view(-1)\n",
    "    item.out_degree = adj.long().sum(dim=0).view(-1)\n",
    "    item.edge_input = torch.from_numpy(edge_input).long()\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "def pad_1d_unsqueeze(x, padlen):\n",
    "    x = x + 1  # pad id = 0\n",
    "    xlen = x.size(0)\n",
    "    if xlen < padlen:\n",
    "        new_x = x.new_zeros([padlen], dtype=x.dtype)\n",
    "        new_x[:xlen] = x\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def pad_2d_unsqueeze(x, padlen):\n",
    "    x = x + 1  # pad id = 0\n",
    "    xlen, xdim = x.size()\n",
    "    if xlen < padlen:\n",
    "        new_x = x.new_zeros([padlen, xdim], dtype=x.dtype)\n",
    "        new_x[:xlen, :] = x\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def pad_attn_bias_unsqueeze(x, padlen):\n",
    "    xlen = x.size(0)\n",
    "    if xlen < padlen:\n",
    "        new_x = x.new_zeros(\n",
    "            [padlen, padlen], dtype=x.dtype).fill_(float('-inf'))\n",
    "        new_x[:xlen, :xlen] = x\n",
    "        new_x[xlen:, :xlen] = 0\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def pad_edge_type_unsqueeze(x, padlen):\n",
    "    xlen = x.size(0)\n",
    "    if xlen < padlen:\n",
    "        new_x = x.new_zeros([padlen, padlen, x.size(-1)], dtype=x.dtype)\n",
    "        new_x[:xlen, :xlen, :] = x\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def pad_rel_pos_unsqueeze(x, padlen):\n",
    "    x = x + 1\n",
    "    xlen = x.size(0)\n",
    "    if xlen < padlen:\n",
    "        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype)\n",
    "        new_x[:xlen, :xlen] = x\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "def pad_3d_unsqueeze(x, padlen1, padlen2, padlen3):\n",
    "    x = x + 1\n",
    "    xlen1, xlen2, xlen3, xlen4 = x.size()\n",
    "    if xlen1 < padlen1 or xlen2 < padlen2 or xlen3 < padlen3:\n",
    "        new_x = x.new_zeros([padlen1, padlen2, padlen3, xlen4], dtype=x.dtype)\n",
    "        new_x[:xlen1, :xlen2, :xlen3, :] = x\n",
    "        x = new_x\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, attn_bias, attn_edge_type, rel_pos, in_degree, out_degree, x, edge_input, y):\n",
    "        super(Batch, self).__init__()\n",
    "        # self.idx = idx\n",
    "        self.in_degree, self.out_degree = in_degree, out_degree\n",
    "        self.x, self.y = x, y\n",
    "        self.attn_bias, self.attn_edge_type, self.rel_pos = attn_bias, attn_edge_type, rel_pos\n",
    "        self.edge_input = edge_input\n",
    "        self.dataset_idx = None\n",
    "\n",
    "    def to(self, device):\n",
    "        # self.idx = self.idx.to(device)\n",
    "        self.in_degree, self.out_degree = self.in_degree.to(\n",
    "            device), self.out_degree.to(device)\n",
    "        self.x, self.y = self.x.to(device), self.y.to(device)\n",
    "        self.attn_bias, self.attn_edge_type, self.rel_pos = self.attn_bias.to(\n",
    "            device), self.attn_edge_type.to(device), self.rel_pos.to(device)\n",
    "        self.edge_input = self.edge_input.to(device)\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.in_degree.size(0)\n",
    "\n",
    "\n",
    "def collator(items, max_node=512, multi_hop_max_dist=20, rel_pos_max=20):\n",
    "    items = [\n",
    "        item for item in items if item is not None and item.x.size(0) <= max_node]\n",
    "    if len(items) == 0:\n",
    "        return None\n",
    "    items = [(item.attn_bias, item.attn_edge_type, item.rel_pos, item.in_degree,\n",
    "              item.out_degree, item.x, item.edge_input[:, :, :multi_hop_max_dist, :], item.y) for item in items]\n",
    "    attn_biases, attn_edge_types, rel_poses, in_degrees, out_degrees, xs, edge_inputs, ys = zip(\n",
    "        *items)\n",
    "\n",
    "    for idx, _ in enumerate(attn_biases):\n",
    "        attn_biases[idx][1:, 1:][rel_poses[idx] >= rel_pos_max] = float('-inf')\n",
    "    max_node_num = max(i.size(0) for i in xs)\n",
    "    max_dist = max(i.size(-2) for i in edge_inputs)\n",
    "    y = torch.cat(ys)\n",
    "    x = torch.cat([pad_2d_unsqueeze(i, max_node_num) for i in xs])\n",
    "    edge_input = torch.cat([pad_3d_unsqueeze(\n",
    "        i, max_node_num, max_node_num, max_dist) for i in edge_inputs])\n",
    "    attn_bias = torch.cat([pad_attn_bias_unsqueeze(\n",
    "        i, max_node_num + 1) for i in attn_biases])\n",
    "    attn_edge_type = torch.cat(\n",
    "        [pad_edge_type_unsqueeze(i, max_node_num) for i in attn_edge_types])\n",
    "    rel_pos = torch.cat([pad_rel_pos_unsqueeze(i, max_node_num)\n",
    "                        for i in rel_poses])\n",
    "    in_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
    "                          for i in in_degrees])\n",
    "    out_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num)\n",
    "                           for i in out_degrees])\n",
    "    return Batch(\n",
    "        # idx=torch.LongTensor(idxs),\n",
    "        attn_bias=attn_bias,\n",
    "        attn_edge_type=attn_edge_type,\n",
    "        rel_pos=rel_pos,\n",
    "        in_degree=in_degree,\n",
    "        out_degree=out_degree,\n",
    "        x=x,\n",
    "        edge_input=edge_input,\n",
    "        y=y,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed12a9b",
   "metadata": {},
   "source": [
    "## Dataset+Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c9329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GraphormerDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "\n",
    "        self.num = len(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.indices = torch.arange(self.num)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # directly return the sampled graph\n",
    "        sampled_graph = self.dataset[self.indices[item]]\n",
    "        return preprocess_item(sampled_graph)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num\n",
    "\n",
    "    def shuffle(self):\n",
    "        rand = torch.randperm(self.num)\n",
    "        self.indices = self.indices[rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class GraphormerDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "\n",
    "        self.dataset = GraphormerDataset(dataset)\n",
    "        self.collator = partial(collator, max_node=128, multi_hop_max_dist=5, rel_pos_max=1024)\n",
    "\n",
    "        kwargs[\"collate_fn\"] = self.__collate_fn__\n",
    "        super().__init__(dataset=self.dataset, **kwargs)\n",
    "\n",
    "    def __collate_fn__(self, batch):\n",
    "        batch_graphs = batch\n",
    "        batch_graphs = self.collator(batch_graphs)  # make the sampled graphs a batch\n",
    "        return batch_graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c27e39",
   "metadata": {},
   "source": [
    "# Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cbd0e",
   "metadata": {},
   "source": [
    "## Graphormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def init_bert_params(module, n_layers):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(n_layers))\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "class Graphormer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers,\n",
    "        num_heads,\n",
    "        hidden_dim,\n",
    "        dropout_rate,\n",
    "        intput_dropout_rate,\n",
    "        ffn_dim,\n",
    "        edge_type,\n",
    "        multi_hop_max_dist,\n",
    "        attention_dropout_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # for simple feature\n",
    "        self.atom_encoder = nn.Embedding(512 * 2 + 1, hidden_dim, padding_idx=0)\n",
    "        self.edge_encoder = nn.Embedding(512 * 2 + 1, num_heads, padding_idx=0)\n",
    "\n",
    "        self.edge_type = edge_type\n",
    "        if self.edge_type == 'multi_hop':\n",
    "            self.edge_dis_encoder = nn.Embedding(128 * num_heads * num_heads, 1)\n",
    "        self.rel_pos_encoder = nn.Embedding(512, num_heads, padding_idx=0)\n",
    "        self.in_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
    "        self.out_degree_encoder = nn.Embedding(512, hidden_dim, padding_idx=0)\n",
    "\n",
    "        self.input_dropout = nn.Dropout(intput_dropout_rate)\n",
    "        encoders = [EncoderLayer(hidden_dim, ffn_dim, dropout_rate, attention_dropout_rate, num_heads)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "        self.final_ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.graph_token = nn.Embedding(1, hidden_dim)\n",
    "        self.graph_token_virtual_distance = nn.Embedding(1, num_heads)\n",
    "\n",
    "        self.multi_hop_max_dist = multi_hop_max_dist\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.apply(lambda module: init_bert_params(module, n_layers=n_layers))\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        attn_bias, rel_pos, x = batched_data.attn_bias, batched_data.rel_pos, batched_data.x\n",
    "        in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n",
    "        edge_input, attn_edge_type = batched_data.edge_input, batched_data.attn_edge_type\n",
    "\n",
    "        # graph_attn_bias\n",
    "        n_graph, n_node = x.size()[:2]\n",
    "        graph_attn_bias = attn_bias.clone()\n",
    "        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n",
    "            1, self.num_heads, 1, 1)  # [n_graph, n_head, n_node+1, n_node+1]\n",
    "\n",
    "        # rel pos\n",
    "        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
    "        rel_pos_bias = self.rel_pos_encoder(rel_pos).permute(0, 3, 1, 2)\n",
    "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + rel_pos_bias  # spatial encoder\n",
    "        # reset rel pos here\n",
    "        t = self.graph_token_virtual_distance.weight.view(1, self.num_heads, 1)\n",
    "        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n",
    "        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n",
    "\n",
    "        # edge feature\n",
    "        if self.edge_type == 'multi_hop':\n",
    "            rel_pos_ = rel_pos.clone()\n",
    "            rel_pos_[rel_pos_ == 0] = 1  # set pad to 1\n",
    "            # set 1 to 1, x > 1 to x - 1\n",
    "            rel_pos_ = torch.where(rel_pos_ > 1, rel_pos_ - 1, rel_pos_)\n",
    "            if self.multi_hop_max_dist > 0:\n",
    "                rel_pos_ = rel_pos_.clamp(0, self.multi_hop_max_dist)\n",
    "                edge_input = edge_input[:, :, :, :self.multi_hop_max_dist, :]\n",
    "            # [n_graph, n_node, n_node, max_dist, n_head]\n",
    "            edge_input = self.edge_encoder(edge_input).mean(-2)\n",
    "            max_dist = edge_input.size(-2)\n",
    "            edge_input_flat = edge_input.permute(\n",
    "                3, 0, 1, 2, 4).reshape(max_dist, -1, self.num_heads)\n",
    "            edge_input_flat = torch.bmm(edge_input_flat, self.edge_dis_encoder.weight.reshape(\n",
    "                -1, self.num_heads, self.num_heads)[:max_dist, :, :])\n",
    "            edge_input = edge_input_flat.reshape(\n",
    "                max_dist, n_graph, n_node, n_node, self.num_heads).permute(1, 2, 3, 0, 4)\n",
    "            edge_input = (edge_input.sum(-2) /\n",
    "                          (rel_pos_.float().unsqueeze(-1))).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n",
    "            edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n",
    "\n",
    "        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input  # edge encoder\n",
    "        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)  # reset\n",
    "\n",
    "        # node feauture + graph token\n",
    "        node_feature = self.atom_encoder(x).sum(dim=-2)  # [n_graph, n_node, n_hidden]\n",
    "\n",
    "        node_feature = node_feature + \\\n",
    "            self.in_degree_encoder(in_degree) + \\\n",
    "            self.out_degree_encoder(out_degree)  # degree encoder\n",
    "        graph_token_feature = self.graph_token.weight.unsqueeze(\n",
    "            0).repeat(n_graph, 1, 1)\n",
    "        graph_node_feature = torch.cat(\n",
    "            [graph_token_feature, node_feature], dim=1)\n",
    "\n",
    "        # transfomrer encoder\n",
    "        output = self.input_dropout(graph_node_feature)\n",
    "        for enc_layer in self.layers:\n",
    "            output = enc_layer(output, attn_bias=graph_attn_bias)\n",
    "        output = self.final_ln(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, ffn_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(ffn_size, hidden_size)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dropout_rate, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.att_size = att_size = hidden_size // num_heads\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
    "\n",
    "        self.input_norm = nn.LayerNorm(hidden_size)\n",
    "        self.output_layer = nn.Linear(num_heads * att_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, attn_bias=None):\n",
    "        orig_q_size = x.size()\n",
    "\n",
    "        x = self.input_norm(x)\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(x).view(batch_size, -1, self.num_heads, d_k)\n",
    "        k = self.linear_k(x).view(batch_size, -1, self.num_heads, d_k)\n",
    "        v = self.linear_v(x).view(batch_size, -1, self.num_heads, d_v)\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q = q * self.scale\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        if attn_bias is not None:\n",
    "            x = x + attn_bias\n",
    "\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.num_heads * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size, dropout_rate, attention_dropout_rate, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, attention_dropout_rate, num_heads)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, ffn_size)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, attn_bias=None):\n",
    "        y = self.self_attn(x, attn_bias)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x1 = x + y\n",
    "        y = self.ffn(x1)\n",
    "        y = self.ffn_dropout(y)\n",
    "        return x1 + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffd1b0",
   "metadata": {},
   "source": [
    "## Decoder for graph-level class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention\n",
    "from torch_geometric.nn.aggr import Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import uniform\n",
    "\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "\n",
    "class NNDecoder(torch.nn.Module):\n",
    "    def __init__(self, num_tasks, emb_dim = 300, graph_pooling = \"mean\"):\n",
    "        super(NNDecoder, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.graph_pooling = graph_pooling\n",
    "        ### Pooling function to generate whole-graph embeddings\n",
    "        if self.graph_pooling == \"sum\":\n",
    "            self.pool = global_add_pool\n",
    "        elif self.graph_pooling == \"mean\":\n",
    "            self.pool = global_mean_pool\n",
    "        elif self.graph_pooling == \"max\":\n",
    "            self.pool = global_max_pool\n",
    "        elif self.graph_pooling == \"attention\":\n",
    "            self.pool = GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, 1)))\n",
    "        elif self.graph_pooling == \"set2set\":\n",
    "            self.pool = Set2Set(emb_dim, processing_steps = 2)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph pooling type.\")\n",
    "\n",
    "        if graph_pooling == \"set2set\":\n",
    "            self.decoder = torch.nn.Linear(2*self.emb_dim, num_tasks)\n",
    "        else:\n",
    "            self.decoder = torch.nn.Linear(self.emb_dim, num_tasks)\n",
    "\n",
    "    def forward(self, node_rep):\n",
    "        h_graph = node_rep[:, 0, :]\n",
    "        return self.decoder(h_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf29db",
   "metadata": {},
   "source": [
    "# Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2222da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from time import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### importing OGB\n",
    "from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
    "from ogb.graphproppred import Evaluator\n",
    "\n",
    "\n",
    "def train(epoch, model_list, device, loader, optimizer_list, task_type):\n",
    "    model, decoder = model_list\n",
    "    optimizer, dec_optimizer = optimizer_list\n",
    "\n",
    "    model.train()\n",
    "    decoder.train()\n",
    "\n",
    "    clf_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    reg_criterion = torch.nn.MSELoss()\n",
    "\n",
    "    loss_list = []\n",
    "    epoch_iter = tqdm(loader, ncols=130)\n",
    "    for step, batch in enumerate(epoch_iter):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        dec_optimizer.zero_grad()\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            node_rep = model(batch)\n",
    "            pred = decoder(node_rep)\n",
    "            ## ignore nan targets (unlabeled) when computing training loss.\n",
    "            is_labeled = batch.y == batch.y\n",
    "            criterion = clf_criterion if \"classification\" in task_type else reg_criterion\n",
    "            loss = criterion(pred.float()[is_labeled], batch.y.float()[is_labeled])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            dec_optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            epoch_iter.set_description(f\"epoch: {epoch}, train_loss: {loss:.4f}\")\n",
    "\n",
    "    return np.mean(loss_list)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model_list, device, loader, evaluator):\n",
    "    model, decoder = model_list\n",
    "\n",
    "    model.eval()\n",
    "    decoder.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            node_rep = model(batch)\n",
    "            pred = decoder(node_rep)\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = \"cpu\"\n",
    "\n",
    "    ### automatic dataloading and splitting\n",
    "    # ct: custom dataset\n",
    "    s = time()\n",
    "\n",
    "    dataset = PygGraphPropPredDataset(name=\"ogbg-molfreesolv\")\n",
    "    split_idx = dataset.get_idx_split()\n",
    "\n",
    "    # only retain the top two node/edge features\n",
    "    dataset.data.x = dataset.data.x[:,:2]\n",
    "\n",
    "\n",
    "    dataset.data.edge_attr = dataset.data.edge_attr[:,:2]\n",
    "\n",
    "    ### automatic evaluator. takes dataset name as input\n",
    "    evaluator = Evaluator(\"ogbg-molfreesolv\")\n",
    "\n",
    "    model = Graphormer(n_layers=3,\n",
    "                            num_heads=2,\n",
    "                            hidden_dim=32,\n",
    "                            dropout_rate=0.1,\n",
    "                            intput_dropout_rate=0.1,\n",
    "                            ffn_dim=32,\n",
    "                            edge_type=\"multi_hop\",\n",
    "                            multi_hop_max_dist=5,\n",
    "                            attention_dropout_rate=0.1,\n",
    "                        ).to(device)\n",
    "\n",
    "    train_loader = GraphormerDataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers = 1)\n",
    "    valid_loader = GraphormerDataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers = 1)\n",
    "    test_loader = GraphormerDataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers = 1)\n",
    "\n",
    "    decoder = NNDecoder(emb_dim = 32, num_tasks = dataset.num_tasks).to(device)\n",
    "    model_list = [model, decoder]\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    dec_optimizer = optim.Adam(decoder.parameters(), lr=1e-4)\n",
    "    optimizer_list = [optimizer, dec_optimizer]\n",
    "\n",
    "    train_curve = []\n",
    "    valid_curve = []\n",
    "    test_curve = []\n",
    "    train_val_curve = []\n",
    "\n",
    "    for epoch in range(1, 51):\n",
    "        train_perf = train(epoch, model_list, device, train_loader, optimizer_list, dataset.task_type)\n",
    "        train_curve.append(train_perf)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "\n",
    "            valid_perf = test(model_list, device, valid_loader, evaluator)\n",
    "            test_perf = test(model_list, device, test_loader, evaluator)\n",
    "\n",
    "            print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf})\n",
    "\n",
    "            valid_curve.append(valid_perf[dataset.eval_metric])\n",
    "            test_curve.append(test_perf[dataset.eval_metric])\n",
    "\n",
    "        else:\n",
    "            print({'Train': train_perf})\n",
    "\n",
    "    if 'classification' in dataset.task_type:\n",
    "        best_val_epoch = np.argmax(np.array(valid_curve))\n",
    "    else:\n",
    "        best_val_epoch = np.argmin(np.array(valid_curve))\n",
    "\n",
    "    print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
    "    print('Test score: {}'.format(test_curve[best_val_epoch]))\n",
    "\n",
    "    return valid_curve[best_val_epoch], test_curve[best_val_epoch]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    val_metric, test_metric = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea3ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aee55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
